\section{DDSP}

\cite{DDSP} used as main reference for the research. \cite{テスト２}

\subsection{Important bullet points}

\begin{itemize}
    \item Interpretable and modular approach to generative modeling
    \item Classic signal processing elements with deep learning methods
    \item Models that rely on strided convolution or windowing (STFT)
    need to align waveshapes or suffer from spectral leakage.
    \item DDSP takes the approach of vocoders in using oscillators to
    synthesize signals.
    \item According to the DDSP paper \cite{DDSP}, the DDSP library is capable
    of extrapolating timbres not seen during training, and independent control
    over pitch and loudness during synthesis.
    \item References to Neural Source Filter \cite{neuralsourcefilter} imply
    this might be a good additional reference for research. \cite{日本テスト}

\end{itemize}

\subsection{Newly learned concepts from this document}

\begin{itemize}
    \item Strided Convolution:
    
    Convolution that has a hop length, meaning, it skips some information
    to avoid analyzing redundancies.

    \item Teacher forcing:
    
    Feeding back the correct answers into training algorithms 
    to reduce training times and lead the model in the right direction during training.

    \item Automatic differentiation:
    
    Also: 'algorithmic differentiation' or 'computational differentiation' means splitting
    derivatives into simpler operations using the chain rule.

    \item Deterministic autoencoder
    \item Adversarial training
    \item Variational inference
    \item Jacobian design
    \item Stochastic latents
    \item CREPE model
    \item Latent encoding
\end{itemize}

\subsection{How it relates to my research}

I believe the general structure and inner workings of the proposed DDSP synthesizer from the paper
are exactly how I need to model my own synthesizer.

The proposed system requires an audio database to train the machine learning model
and then use that model to extract the required features to build a faithful additive
synthesizer. What's interesting about this system is that it allows for the creation
of timbres beyond the training set, meaning that theoretically, if my training database
consists of a collection of plucked string samples, I could interpolate timbres between them
or exaggerate some of their features beyond what I have already recorded.

Since this model has been tested with longer audio samples,
so far a lot of the development has been using audio sampled at 
relatively low sample rates (in the range of the 16kHz sample rate).
However, due to the narrow scope of my project, I believe I should be able to use
samples recorded at higher sample rates and generate high-quality output.

Another interesting aspect of the DDSP library is that it allows for 
flexible scalability. Meaning, I could generate consistent output with very few
parameters and relatively low computational power.